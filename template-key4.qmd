---
title: "ESM 244 Week 8: Principal Components Analysis Key"
author: "Nathaniel Grimes"
date: "2025-01-30"
format: 
  html:
    embed-resources: true
    code-fold: true
    toc: true
execute:
  warning: false
  message: false
---

```{r setup}

library(tidyverse)
library(here)

library(ggfortify) # For PCA biplot

```

# Principal components analysis (PCA)

Principal components analysis is an ordination method allowing us to glean as much about our multivariate data as possible in a simplified number of dimensions.

Here, we'll use [data from the Human Development Index](https://hdr.undp.org/data-center).  Quick overview of the HDI:

> The Human Development Index (HDI) is a summary measure of average achievement in key dimensions of human development: a long and healthy life, being knowledgeable and having a decent standard of living. The HDI is the geometric mean of normalized indices for each of the three dimensions.

> The health dimension is assessed by life expectancy at birth, the education dimension is measured by mean of years of schooling for adults aged 25 years and more and expected years of schooling for children of school entering age. The standard of living dimension is measured by gross national income per capita. The HDI uses the logarithm of income, to reflect the diminishing importance of income with increasing GNI. The scores for the three HDI dimension indices are then aggregated into a composite index using geometric mean. Refer to Technical notes for more details.


Load in the data using `read_csv`. Perform some exploratory analysis on the data to get a better grasp of the structure. What do we need for pca to work? What variable needs to be transformed according to the information presented above?

**PCA requires continuous numeric data with no NAs.  So we must drop categorical and character data, and exclude any rows with NAs.  We should also rescale so all numeric variables have a mean 0 and sd 1.**

```{r}
hdi_data_raw <- read_csv(here('data','hdi_clean.csv')) 

# glimpse(hdi_data_raw)
# summary(hdi_data_raw)

hdi_data_long <- hdi_data_raw %>%
  pivot_longer(names_to = 'name', values_to = 'value', where(is.numeric))

ggplot(hdi_data_long, aes(x = value)) +
  geom_histogram() +
  facet_wrap(~ name, scales = 'free_x')
```

```{r}
hdi_data <- hdi_data_raw %>%
  drop_na() %>%
  mutate(hdicode = factor(hdicode, levels = c('Low', 'Medium', 'High', 'Very High'))) %>%
  mutate(gnipc_2021 = log(gnipc_2021))

hdi_pca <- hdi_data %>% 
  select(where(is.numeric)) %>%
  # select(-iso3, -country, -hdicode)) %>%  other ways of doing the select
  # select(ends_with('2021'), lat) %>%
  prcomp(scale = TRUE)
```


Examine the structure of the hdi_pca object. What do you think each piece means? Use documentation to help you understand the output.

* `sdev` is the standard deviation (square root of explained variance captured by each principal component)
* `rotation` is the translation of base variables to principal components (and vice versa)
* `center` is the factor applied for centering of the variables
* `scale` is the factor for rescaling of the variables
* `x` is a dataframe of all observations transformed to their new values in PC1, 2, etc.


``` {r}
# See the loadings (weighting for each principal component)
hdi_pca$rotation
```

What can we say about the contributions of these variables to PC1, PC2, and PC3?

## Scree Plots

Let's make the screeplot by hand. First, what is the variable of interest on the y-axis and x-axis in a screeplot? How do we extract this information from the PCA object?

Create a dataframe with the necessary indgreidents to make a screeplot. One piece that may not be immediately obvious is provided to get you started. We'll need to keep track of which PC is which, `colnames(hdi_pca$rotation)` will give you the order of the PCs.

```{r}
pc_names <- colnames(hdi_pca$rotation)
sd_vec <- hdi_pca$sdev
var_vec <- sd_vec^2 ### standard deviation is sqrt of variance!


pct_expl_df <- data.frame(v = var_vec,
                          pct_v = var_vec / sum(var_vec),
                          pc = pc_names)

# Screeplot
ggplot(pct_expl_df, aes(x = pc, y = pct_v)) +
  geom_col() +
  labs(x = 'Principal component', y = 'Variance explained')+
  scale_y_continuous(labels = scales::percent,expand = c(0,0))

# Showing another way where we add the percentage explained as labels
ggplot(pct_expl_df, aes(x = pc, y = v)) +
  geom_col() +
  geom_text(aes(label = scales::percent(pct_v)), vjust = 0, nudge_y = .002) +
  labs(x = 'Principal component', y = 'Variance explained')
```


We can use ggfortify to create a screeplot as well.  This is a bit more automated, but less flexible. Great for diagnostics, but not for publication.

```{r}
# Variance explained by each PC
screeplot(hdi_pca, type = "lines")
screeplot(hdi_pca, type = "barplot")
```

## ggfortify autoplots

Autoplot can take a PCA object the original dataframe, and plot the observations in the new PC space.  It can also plot the loadings of the variables. Run the code chunk first to see the biplot of HDI data.

One cool feature of ggfortify plots is that they follow the same graphics grammar after the initial plot is created.  So you can add layers, change themes, etc. Clean up the biplot to make it more presentable. (Hint: use the data)


``` {r}
#| label: fig-biplot
#| fig-cap: There appears to be a distinct grouping of highly developed countries in the positive first component direction.

autoplot(hdi_pca,
     	data = hdi_data,
     	loadings = TRUE,
     	colour = 'hdicode',
     	loadings.label = TRUE,
     	loadings.colour = "black",
     	loadings.label.colour = "black",
     	loadings.label.vjust = -0.5
     	) +
  scale_color_manual(values = c('red', 'orange', 'yellowgreen', 'darkgreen')) +
  theme_minimal()

# It's not perfect, but it's enough for now...
```


## Tidymodels

This is a quick add on to show how to work pca into tidymodels. The reason I didn't teach it this way was because I didn't want to explain what the difference between prepped and baked receipes. Also there's no easy way to extract pca rotations, loadings, or making biplots yet in tidymodels. Tidymodels is meant for preparing the data into use for more complicated ML algorithms. Like you would use the PC1 to be a branch in a random forest. 

PCA is done all in the `receipes`.

```{r}
library(tidymodels)

pca_rec<-recipe(~.,data=hdi_data_raw) |> 
  update_role(iso3,new_role = 'id') |> 
  step_naomit() |> 
  step_normalize(all_numeric_predictors()) |>
  step_pca(all_numeric_predictors())

# prep the receipe (aka acutally run allthe steps)

pca_prep<-prep(pca_rec)
```


Let's look at the top 5 most influential variables in the first 4 PCs.

```{r}
tidy_pca<-tidy(pca_prep,3)  # 3 because it's our 3rd step

library(tidytext)

tidy_pca |> 
  filter(component %in% paste0("PC", 1:4)) |> 
  group_by(component) |> 
  top_n(5, abs(value)) |> 
  ungroup() |> 
  mutate(terms=reorder_within(terms, abs(value),component)) |> 
  ggplot(aes(abs(value),terms,fill=value>0))+
  geom_col()+
  facet_wrap(~component, scales='free_y')+
  scale_y_reordered()+
  labs(
    x='Absolute value of contribution',
    y=NULL,
    fill='Positive?'
  )+
  theme_minimal()
```

The inequality measures are the biggest drive of variation in the first principal component, but some are positive and negative. If we compare to our biplot in @fig-biplot this checks out. the longest vectors in the x direction are the inequality ones.

I can easily plot the points by `juicing` the receipe, but I still have to manually create the loading vectors. Honestly it's a pain and we get the same answer as before.

```{r}
juice(pca_prep) |> 
  ggplot(aes(PC1,PC2))+
  geom_point(aes(color=hdicode),alpha=0.7,size=2)
```

Looks the same as above, just with different colors and without the loading vectors. Tidymodel PCAs are great for passing into more complicated models with the receipe framework, just not conducive if you want to focus on the pca itself.

